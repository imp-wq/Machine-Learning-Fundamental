+ 在深度学习领域，卷积核是不确定的
	![[Pasted image 20250721181346.png]]
- 将传统neural network中一个全连接层替换成卷积层
- 池化层：对卷积层后的特征图像进行降维，减少计算量同时保留主要特征
- 卷积层，池化层，全连接层可以有多个
	![[Pasted image 20250721182004.png]]
- 我们可以观察出卷积层从图像中提取出了什么特征
### Wording Embedding
- 是通过深度学习方法训练出来的
- 潜空间：词向量所在的空间，通常维度很高
### Transformer
- RNN：
	- 将第一个词经过非线性变换得到隐藏状态h1，加入第二个词的计算。
		以在第二个词的编码中保留第一个词的信息。
	![[Pasted image 20250722130555.png]]

	- 有多个权重W：
		- 针对词向量的$W_{xh}$
		- 针对隐藏状态的$W_{hh}$
		- 计算输出结果的$W_{hy}$矩阵
	- 同样的，也有多个偏置项b
		![[Pasted image 20250722145024.png]]

- 过程：
	![[Pasted image 20250722152511.png]]
+ 和经典的neural network，主要区别在于加了前一时刻的隐藏状态：
	![[Pasted image 20250722152555.png]]
 - RNN的问题：
	1. 信息会随着时间步的增多而逐渐丢失，无法捕捉长期依赖
	2. 无法并行计算
- 注意力机制Attention：
	每个词都把其他词的词向量按照和自己的相似度权重加到自己的了自己的词向量中。
	从而让每个词向量都包含位置信息和其他词上下文信息。
- Multi-head Attention
- 






















