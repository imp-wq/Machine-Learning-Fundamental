- [Attention for Neural Networks, Clearly Explained!!!](https://www.youtube.com/watch?v=PSs6nxngL6k&t=194s)
- [NLP Course | For You](https://lena-voita.github.io/nlp_course.html)
- [Lightning AI](https://lightning.ai/)

- Main idea of Attention:
	- sometimes the first word matters
	- add a bunch of new paths from the Encoder to the Decode, one per input value, so that each step of the Decoder can directly access input values.
	- 